"""
This script parses the raw log files generated by the training runs and
compiles the key metrics into a clean, well-formatted CSV file suitable for analysis.
This is the robust, reproducible way to generate our final results dataset.
"""
import argparse
import csv
import re
from pathlib import Path


def parse_log_file(file_path: Path) -> list[dict]:
    """Extract structured epoch data from a single log file."""
    run_name = (
        file_path.stem.replace('_', ' ')
        .replace(' full', '')
        .replace(' no ortho', '-no-ortho')
        .title()
        .replace(' ', '-')
    )

    train_pattern = re.compile(
        r"epoch (\d+) \| train_loss=([\d.eE+-]+) \| train_mae=([\d.eE+-]+)"
    )
    val_pattern = re.compile(
        r"epoch (\d+) \| val_mae=([\d.eE+-]+) \| val_rmse=([\d.eE+-]+)"         r"(?: \| val_mae_rattle \(low/med/high\)=([\d.eE+-]+)/([\d.eE+-]+)/([\d.eE+-]+))?"
    )

    epochs: dict[int, dict[str, float]] = {}
    for line in file_path.read_text().splitlines():
        train_match = train_pattern.search(line)
        if train_match:
            epoch = int(train_match.group(1))
            data = epochs.setdefault(
                epoch,
                {
                    "run": run_name,
                    "epoch": epoch,
                    "train_loss": float('nan'),
                    "train_mae": float('nan'),
                    "val_mae": float('nan'),
                    "val_rmse": float('nan'),
                    "val_mae_low_rattle": float('nan'),
                    "val_mae_medium_rattle": float('nan'),
                    "val_mae_high_rattle": float('nan'),
                },
            )
            data["train_loss"] = float(train_match.group(2))
            data["train_mae"] = float(train_match.group(3))
            continue

        val_match = val_pattern.search(line)
        if val_match:
            epoch = int(val_match.group(1))
            data = epochs.setdefault(
                epoch,
                {
                    "run": run_name,
                    "epoch": epoch,
                    "train_loss": float('nan'),
                    "train_mae": float('nan'),
                    "val_mae": float('nan'),
                    "val_rmse": float('nan'),
                    "val_mae_low_rattle": float('nan'),
                    "val_mae_medium_rattle": float('nan'),
                    "val_mae_high_rattle": float('nan'),
                },
            )
            data["val_mae"] = float(val_match.group(2))
            data["val_rmse"] = float(val_match.group(3))
            if val_match.group(4):
                data["val_mae_low_rattle"] = float(val_match.group(4))
                data["val_mae_medium_rattle"] = float(val_match.group(5))
                data["val_mae_high_rattle"] = float(val_match.group(6))

    return sorted(epochs.values(), key=lambda row: row["epoch"]) if epochs else []


def main() -> None:
    """Find logs, parse them, and write a clean CSV."""
    parser = argparse.ArgumentParser(description="Parse training logs into a clean CSV.")
    parser.add_argument(
        "--log-dir",
        type=Path,
        default=Path("results"),
        help="Directory containing the log files.",
    )
    parser.add_argument(
        "--output-file",
        type=Path,
        default=Path("results/metrics_clean.csv"),
        help="Path to the output CSV file.",
    )
    parser.add_argument(
        "--pattern",
        type=str,
        default="*.log",
        help="Glob pattern for log files (default: *.log).",
    )
    args = parser.parse_args()

    log_files = list(args.log_dir.glob(args.pattern))
    if not log_files:
        print(f"Error: No files matching {args.pattern} found in {args.log_dir}")
        return

    print(f"Found {len(log_files)} log files to parse...")

    all_data = []
    for log_file in log_files:
        print(f"  - Parsing {log_file.name}...")
        records = parse_log_file(log_file)
        if records:
            all_data.extend(records)

    if not all_data:
        print("Error: No valid epoch data could be parsed from the log files.")
        return

    header = all_data[0].keys()
    with args.output_file.open("w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=header)
        writer.writeheader()
        writer.writerows(all_data)

    print(f"Successfully created clean metrics file at: {args.output_file}")


if __name__ == "__main__":
    main()
